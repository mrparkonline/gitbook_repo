# Bits & Binary

{% embed url="https://www.youtube.com/watch?v=gs4Sb4ar4qw" %}
Harvard CS50 - Lecture 0 Binary Bulbs
{% endembed %}

## Bit

In computing, a "bit" is the smallest unit of digital information and binary data representation. The term "bit" is a contraction of "binary digit." It can have one of two values: 0 or 1, which correspond to binary code for off and on, respectively. Bits serve as the fundamental building blocks for all digital data and are used to represent and process information in computers, including numbers, text, images, and more complex data structures. Collectively, a group of 8 bits forms a "byte," which is often the basic unit for representing characters or values in computer systems.

## Binary Number System

A binary number system is a base-2 numeral system used in mathematics and computing. Unlike the familiar decimal system (base 10), which uses 10 digits (0 through 9), the binary system uses only two digits, 0 and 1. Each digit in a binary number represents a power of 2, with the rightmost digit representing 2^0 (1), the next digit to the left representing 2^1 (2), the next representing 2^2 (4), and so on.

Binary numbers are used extensively in digital electronics and computing because they are well-suited for representing the on-off states of electronic switches (0 for off and 1 for on). In binary, complex data like numbers, text, and images are represented as sequences of 0s and 1s, making it the fundamental language of computers. Additionally, binary operations (e.g., addition, subtraction, multiplication, division) are simpler and more efficient in electronic circuits, which is why computers use binary internally.

{% hint style="info" %}
#### 64 Bit Computing System

A 64-bit system is a type of computer architecture that processes data and instructions using 64-bit binary digits, offering advantages like increased memory capacity (up to millions of terabytes), enhanced computational power, and compatibility with both 32-bit and 64-bit software, making it well-suited for memory-intensive and performance-critical tasks.
{% endhint %}

### ASCII Table

<figure><img src="https://web.alfredstate.edu/faculty/weimandn/miscellaneous/ascii/ASCII%20Conversion%20Chart.gif" alt=""><figcaption><p><a href="https://web.alfredstate.edu/faculty/weimandn/miscellaneous/ascii/ascii_index.html">The ASCII Table</a></p></figcaption></figure>

The ASCII (American Standard Code for Information Interchange) table is a character encoding standard used in computing and communication to represent text and control characters as binary numbers. In the ASCII table, each character is assigned a unique numerical value between 0 and 127, which corresponds to its binary representation.&#x20;

This encoding allows computers to store, transmit, and process text in a consistent and standardized way, making it a fundamental component of character encoding in computing. The first 32 characters in the ASCII table are control characters, such as line feed, carriage return, and tab, while the remaining characters represent printable characters like letters, numbers, and symbols. ASCII is the basis for many other character encodings and remains widely used in computer systems and programming today.

